experiment:
  name: "tprimat_benchmark"
  description: "LLM training benchmark comparing AMD vs NVIDIA hardware"
  version: "1.0"

hardware:
  platforms:
    nvidia:
      gpu_model: "H100"
      memory_per_gpu_gb: 80
      num_gpus: 8
      software_stack: "cuda"
      framework: "nemo"
    
    amd:
      gpu_model: "MI300X"
      memory_per_gpu_gb: 192
      num_gpus: 8
      software_stack: "rocm"
      framework: "primus"

models:
  llama:
    name: "llama3.1_8b"
    full_name: "Llama 3.1 8B"
    num_parameters: 8.0e9
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    primus_config: "examples/megatron/configs/MI300X/llama3.1_8B-pretrain.yaml"
    nemo_recipe: "llama31_8b.pretrain_recipe"
    
  qwen:
    name: "qwen2.5_7b"
    full_name: "Qwen 2.5 7B"
    num_parameters: 7.6e9
    num_layers: 28
    hidden_size: 3584
    num_attention_heads: 28
    primus_config: "examples/megatron/configs/MI300X/qwen2.5_7B-pretrain.yaml"
    nemo_recipe: "qwen25_7b.pretrain_recipe"

training:
  general:
    seed: 42                      # Random seed for reproducibility (ensures same init across runs)
  data:
    micro_batch_size: 1           # Per-GPU batch size per accumulation step
    global_batch_size: 128        # Total batch size across all GPUs
    seq_length: 2048              # Sequence length in tokens
  duration:
    max_steps: 50                 # Number of training steps
    train_iters: 50               # Alternative name used by Primus
  optimizer:
    type: "adam"
    learning_rate: 3.0e-4
    warmup_steps: 10              # Warmup for 20% of training (5/25)
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
  precision:
    default: "bf16"               # Options: fp32, fp16, bf16, fp8
    fp8_hybrid: false             # Enable FP8 hybrid mode (H100 specific)
    fp8_param: false              # Store parameters in FP8
  checkpointing:
    enabled: false
    save_interval: 1000
    keep_last_n: 2

parallelism:
  maximum_performance: # 00
    llama:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 2            # Calculated: 8/(4*1) = 2
        gradient_accumulation_steps: 64  # Calculated: 128/(1*2) = 64
      amd:
        tensor_model_parallel_size: 1    # TP=1 (leverage 192GB memory)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
    qwen:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 1            # Calculated: 8/(4*2) = 1
        gradient_accumulation_steps: 128 # Calculated: 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 1    # TP=1 (leverage 192GB memory)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
  
  identical_config: # 01
    llama:
      nvidia:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
        data_parallel_size: 2
        gradient_accumulation_steps: 64
      amd:
        tensor_model_parallel_size: 4    
        pipeline_model_parallel_size: 1  
        data_parallel_size: 2            
        gradient_accumulation_steps: 64  
    qwen:
      nvidia:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 2
        data_parallel_size: 1
        gradient_accumulation_steps: 128
      amd:
        tensor_model_parallel_size: 4    
        pipeline_model_parallel_size: 2  
        data_parallel_size: 1            
        gradient_accumulation_steps: 128 
  
  memory_optimized: # 02
    # Note: TP must divide attention heads evenly
    # Llama 3.1 8B: 32 heads (divisible by 1,2,4,8,16,32)
    # Qwen 2.5 7B: 28 heads (divisible by 1,2,4,7,14,28)
    # Using TP=4 for compatibility with both models
    llama:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4 (balanced: reduces memory, avoids excessive communication)
        pipeline_model_parallel_size: 2  # PP=2 (further reduce memory per GPU)
        data_parallel_size: 1            # Calculated: 8/(4*2) = 1
        gradient_accumulation_steps: 128 # Calculated: 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 2    # TP=2 (conservative given 192GB)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # Calculated: 8/(2*1) = 4
        gradient_accumulation_steps: 32  # Calculated: 128/(1*4) = 32
    qwen:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4 (max compatible with 28 heads)
        pipeline_model_parallel_size: 2  # PP=2 (further reduce memory per GPU)
        data_parallel_size: 1            # Calculated: 8/(4*2) = 1
        gradient_accumulation_steps: 128 # Calculated: 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 2    # TP=2 (28 heads / 2 = 14 heads per GPU)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # Calculated: 8/(2*1) = 4
        gradient_accumulation_steps: 32  # Calculated: 128/(1*4) = 32
  
  minimal_communication: # 03
    llama:
      nvidia:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel communication)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
      amd:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel communication)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
    qwen:
      nvidia:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel communication)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
      amd:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel communication)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # Calculated: 8/(1*1) = 8
        gradient_accumulation_steps: 16  # Calculated: 128/(1*8) = 16
  
  balanced: # 04
    llama:
      nvidia:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # Calculated: 8/(2*1) = 4
        gradient_accumulation_steps: 32  # Calculated: 128/(1*4) = 32
      amd:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # Calculated: 8/(2*1) = 4
        gradient_accumulation_steps: 32  # Calculated: 128/(1*4) = 32
    qwen:
      nvidia:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 2            # Calculated: 8/(2*2) = 2
        gradient_accumulation_steps: 64  # Calculated: 128/(1*2) = 64
      amd:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 2            # Calculated: 8/(2*2) = 2
        gradient_accumulation_steps: 64  # Calculated: 128/(1*2) = 64

platform_optimizations:
  nvidia:
    precision: "fp8"
    fp8_hybrid: true
    fp8_param: true
    cuda_alloc_conf: "expandable_segments:True"
    activation_checkpointing: true
    gradient_checkpointing: false
    
  amd:
    precision: "bf16"
    fp8_hybrid: false
    fp8_param: false
    activation_checkpointing: false
    gradient_checkpointing: false

benchmarking:
  output:
    directory: "./output"
    filename_format: "benchmark_{software_stack}_{model}.json"
    log_format: "training_{model}.log"
    
  metrics:
    performance:
      - "avg_step_time_seconds"
      - "tokens_per_second"
      - "tokens_per_second_per_gpu"
      - "steps_per_second"
      - "samples_per_second"
    
    memory:
      - "avg_memory_allocated_gb"
      - "peak_memory_allocated_gb"
      - "avg_memory_reserved_gb"
      - "peak_memory_reserved_gb"
    
    stability:
      - "step_time_std_dev"
      - "step_time_variance"
      - "min_step_time"
      - "max_step_time"
    
    system:
      - "device_name"
      - "device_count"
      - "gpu_cores"
      - "software_stack"
      - "software_version"
      - "pytorch_version"
  
  enhanced_metrics:
    enabled: true
    cloud_costs:
      nvidia_h100_8gpu_per_hour: 32.0
      amd_mi300x_8gpu_per_hour: 24.0
    hardware_specs:
      nvidia_h100:
        peak_tflops_fp8: 989.0e12
        peak_tflops_fp16: 494.0e12
        peak_tflops_bf16: 494.0e12
        tdp_watts: 700
      amd_mi300x:
        peak_tflops_fp16: 653.0e12
        peak_tflops_bf16: 653.0e12
        peak_tflops_fp8: 1300.0e12
        tdp_watts: 750
  
  warmup:
    skip_first_n_steps: 1         # Skip first step (warmup)
    
  runs:
    default_num_runs: 1           # For quick benchmarking
    recommended_num_runs: 3       # For statistical significance
    cooldown_seconds: 10          # Between runs

comparison:
  metrics_to_compare:
    - "tokens_per_second_per_gpu"
    - "avg_step_time_seconds"
    - "avg_memory_allocated_gb"
    - "mfu_percent"
    - "cost_per_trillion_tokens"
  plots:
    format: "png"
    dpi: 300
    style: "seaborn"
    filename: "compare.png"
  report:
    format: "markdown"
    filename: "comparison_report.md"
    include_plots: true

paths:
  primus:
    installation: "${PRIMUS_PATH:-/workspace/Primus}"
    config_dir: "examples/megatron/configs/MI300X"
    run_script: "./examples/run_pretrain.sh"
  nemo:
    checkpoint_dir: "/checkpoints"
  tprimat:
    base_dir: "."
    output_dir: "./output"
    logs_dir: "./output"

logging:
  console:
    enabled: true
    level: "INFO"
    colored: true
  file:
    enabled: true
    capture_stdout: true
    capture_stderr: true
  disable:
    tensorboard: true
    wandb: true
    mlflow: true

profiling:
  enabled: false                    # Enable Kineto profiling
  export_chrome_trace: true         # Export Chrome trace JSON
  profile_memory: true              # Track memory allocations
  with_stack: true                  # Include Python stack traces
  with_flops: true                  # Estimate FLOPs
  record_shapes: true               # Record tensor shapes
  schedule:
    wait: 1                         # Skip first N steps (warmup)
    warmup: 1                       # Warmup for N steps
    active: 5                       # Profile N steps
    repeat: 1                       # Repeat cycle N times
