diff --git a/primus/backends/megatron/patches/training_log_patches.py b/primus/backends/megatron/patches/training_log_patches.py
index e19d7dc..3182712 100644
--- a/primus/backends/megatron/patches/training_log_patches.py
+++ b/primus/backends/megatron/patches/training_log_patches.py
@@ -94,6 +94,7 @@ class RocmMonitorExtension:
         fragile dependency on 'training_log' argument positions.
         """
         self.call_count += 1
+        log_rank_all(f"THP: call_count={self.call_count}")
 
     def __enter__(self):
         """Swap print_rank_last with our enhanced version."""
@@ -145,6 +146,7 @@ class RocmMonitorExtension:
             hip_free, hip_total = torch.cuda.mem_get_info()
             hip_used = hip_total - hip_free
             hip_ratio = hip_used / hip_total
+            log_rank_all(f"THP: hip_free={hip_free}, hip_total={hip_total}, hip_used={hip_used}, hip_ratio={hip_ratio}")
             hip_mem_str = (
                 f" hip mem usage/free/total/usage_ratio: "
                 f"{hip_used / 1024 ** 3:.2f}GB/"
@@ -162,12 +164,15 @@ class RocmMonitorExtension:
         # collected value, reuse the last known ROCm SMI stats to keep the log
         # informative without incurring per-step overhead.
         should_collect_smi = self.use_rocm_mem or (self.call_count in self.rocm_iters)
+        log_rank_all(f"THP: should_collect_smi={should_collect_smi}")
 
         if should_collect_smi:
             try:
                 local_rank = torch.cuda.current_device()
+                log_rank_all(f"THP: local_rank={local_rank}")
                 r_total, r_used, r_free = get_rocm_smi_mem_info(local_rank)
                 r_ratio = r_used / r_total
+                log_rank_all(f"THP: r_total={r_total}, r_used={r_used}, r_free={r_free}, r_ratio={r_ratio}")
                 rocm_mem_str = (
                     f" rocm mem usage/free/total/usage_ratio: "
                     f"{r_used / 1024 ** 3:.2f}GB/"
@@ -207,6 +212,7 @@ def _wrap_training_log_with_extensions(
         - Attaches Primus metadata to enable future stacking.
     """
     all_extensions = list(existing_extensions) + list(new_extensions)
+    log_rank_all(f"THP: len(existing_extensions)={len(existing_extensions)}, len(new_extensions)={len(new_extensions)}, len(all_extensions)={len(all_extensions)}")
 
     def _patched_training_log(*func_args, **func_kwargs):
         # Use ExitStack to manage multiple extensions dynamically
@@ -301,6 +307,7 @@ def patch_training_log_unified(ctx: PatchContext):
         # -> ROCm Memory Monitoring
         use_rocm_mem = config.get("use_rocm_mem_info", False)
         rocm_iters = config.get("use_rocm_mem_info_iters", [])
+        log_rank_all(f"THP: len(rocm_iters)={len(rocm_iters)}")
 
         enable_rocm_stats = (
             hasattr(args, "log_throughput")
@@ -319,14 +326,17 @@ def patch_training_log_unified(ctx: PatchContext):
         is_wrapped, original_fn, existing_extensions = _get_training_log_wrapper_state(
             megatron_training.training_log
         )
+        log_rank_all(f"THP: len(existing_extensions)={len(existing_extensions)}, len(new_extensions)={len(new_extensions)}")
 
         if not new_extensions:
             if is_wrapped:
                 # We don't remove existing extensions; just log and keep them.
+                existing_count = len(existing_extensions)
+                log_rank_all(f"THP: existing_count={existing_count} (no new extensions case)")
                 log_rank_0(
                     "[Patch:megatron.training_log][INFO] "
                     "No new extensions; keeping existing wrapper "
-                    f"({len(existing_extensions)} existing extensions)"
+                    f"({existing_count} existing extensions)"
                 )
             else:
                 log_rank_0(
@@ -344,11 +354,13 @@ def patch_training_log_unified(ctx: PatchContext):
         # 5. Install the new wrapper
         megatron_training.training_log = patched_fn
 
+        total_extensions = len(existing_extensions) + len(new_extensions)
+        log_rank_all(f"THP: total_extensions={total_extensions}")
         log_rank_0(
             "[Patch:megatron.training_log] Applied wrapper: "
             f"{len(existing_extensions)} existing extensions + "
             f"{len(new_extensions)} new = "
-            f"{len(existing_extensions) + len(new_extensions)} total"
+            f"{total_extensions} total"
         )
 
         # NOTE:
diff --git a/primus/modules/trainer/megatron/trainer.py b/primus/modules/trainer/megatron/trainer.py
index 10152c6..e808052 100644
--- a/primus/modules/trainer/megatron/trainer.py
+++ b/primus/modules/trainer/megatron/trainer.py
@@ -2449,6 +2449,7 @@ class MegatronTrainer(BaseTrainer, BaseModule):
                     rocm_total_mem, rocm_used_mem, rocm_free_mem = get_rocm_smi_mem_info(
                         self.module_local_rank
                     )
+                    log_rank_0(f"THP: iteration={iteration}, rocm_total_mem={rocm_total_mem}, rocm_used_mem={rocm_used_mem}, rocm_free_mem={rocm_free_mem}")
 
             elapsed_time = timers("interval-time").elapsed(barrier=True)
             elapsed_time_per_iteration = elapsed_time / total_iterations
@@ -2461,6 +2462,7 @@ class MegatronTrainer(BaseTrainer, BaseModule):
             throughput = flops_calc(args, batch_size) / (
                 elapsed_time_per_iteration * 10**12 * args.world_size
             )
+            log_rank_0(f"THP: iteration={iteration}, elapsed_time={elapsed_time}, elapsed_time_per_iteration={elapsed_time_per_iteration}, throughput={throughput}, batch_size={batch_size}")
 
             if args.log_timers_to_tensorboard:
                 if writer:
@@ -2497,6 +2499,7 @@ class MegatronTrainer(BaseTrainer, BaseModule):
                     hip_free_mem, hip_total_mem = torch.cuda.mem_get_info()
                     hip_used_mem = hip_total_mem - hip_free_mem
                     hip_mem_usage = hip_used_mem / hip_total_mem
+                    log_rank_0(f"THP: iteration={iteration}, hip_free_mem={hip_free_mem}, hip_total_mem={hip_total_mem}, hip_used_mem={hip_used_mem}, hip_mem_usage={hip_mem_usage}")
                     log_string += (
                         f" hip mem usage/free/total/usage_ratio: {hip_used_mem/1024/1024/1024:.2f}GiB/"
                     )
@@ -2505,6 +2508,7 @@ class MegatronTrainer(BaseTrainer, BaseModule):
 
                 if args.use_rocm_mem_info or iteration in args.use_rocm_mem_info_iters:
                     rocm_mem_usage = rocm_used_mem / rocm_total_mem
+                    log_rank_0(f"THP: iteration={iteration}, rocm_mem_usage={rocm_mem_usage}")
 
                     # get the max rocm_mem_usage
                     usage_tensor = torch.tensor([rocm_mem_usage], device="cuda", dtype=torch.float32)
@@ -2515,6 +2519,7 @@ class MegatronTrainer(BaseTrainer, BaseModule):
                     rocm_mem_usages = [t.item() for t in gathered_usage]
                     max_usage = max(rocm_mem_usages)
                     max_rank = rocm_mem_usages.index(max_usage)
+                    log_rank_0(f"THP: iteration={iteration}, max_usage={max_usage}, max_rank={max_rank}, world_size={world_size}")
 
                     log_string += (
                         f" rocm mem usage/free/total/usage_ratio: {rocm_used_mem/1024/1024/1024:.2f}GiB/"
diff --git a/third_party/Megatron-LM b/third_party/Megatron-LM
--- a/third_party/Megatron-LM
+++ b/third_party/Megatron-LM
@@ -1 +1 @@
-Subproject commit 847781764fe468c90caec16309deded245c1022c
+Subproject commit 847781764fe468c90caec16309deded245c1022c-dirty
