experiment:
  name: "tprimat_benchmark"
  description: "LLM training benchmark comparing AMD vs NVIDIA hardware"
  version: "1.0"

hardware:
  platforms:
    nvidia:
      gpu_model: "H100"
      memory_per_gpu_gb: 80
      num_gpus: 8
      software_stack: "cuda"
      framework: "nemo"
    
    amd:
      gpu_model: "MI300X"
      memory_per_gpu_gb: 192
      num_gpus: 8
      software_stack: "rocm"
      framework: "primus"

models:
  llama:
    name: "llama3.1_8b"
    full_name: "Llama 3.1 8B"
    num_parameters: 8.0e9
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    nemo_recipe: "llama31_8b.pretrain_recipe"
    primus_recipe: "llama3.1_8B-pretrain.yaml"
    
  qwen:
    name: "qwen2.5_7b"
    full_name: "Qwen 2.5 7B"
    num_parameters: 7.6e9
    num_layers: 28
    hidden_size: 3584
    num_attention_heads: 28
    primus_config: "examples/megatron/configs/MI300X/qwen2.5_7B-pretrain.yaml"
    nemo_recipe: "qwen25_7b.pretrain_recipe"

training:
  general:
    seed: 42                        # Random seed for reproducibility
  data:
    micro_batch_size: 1             # Per-GPU batch size per accumulation step
    global_batch_size: 128          # Total batch size across all GPUs
    seq_length: 2048                # Sequence length in tokens
    dataset_path: "/data/llama_dataset_text_document"
    tokenizer_path: "meta-llama/Llama-3.1-8B"
  duration:
    max_steps: 10                   # Number of training steps
    train_iters: 10                 # Alternative name used by Primus
  optimizer:
    type: "adam"
    learning_rate: 3.0e-4
    warmup_steps: 2                # Warmup steps (20% of training)
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
  precision:
    default: "bf16"                 # Options: fp32, fp16, bf16, fp8
    fp8_hybrid: false               # Enable FP8 hybrid mode (H100 specific)
    fp8_param: false                # Store parameters in FP8
  checkpointing:
    enabled: false
    save_interval: 1000
    keep_last_n: 2

parallelism:
  # Maximum Performance: Platform-specific tuning to maximize throughput.
  # NVIDIA: Higher TP to fit model in 80GB, balanced with DP for throughput.
  # AMD: Leverages 192GB memory with TP=1 for max DP, minimal communication.
  # Trade-off: Different settings per platform = non-identical compute paths.
  maximum_performance: # 00
    llama:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 2            # 8/(4*1) = 2
        gradient_accumulation_steps: 64  # 128/(1*2) = 64
      amd:
        tensor_model_parallel_size: 1    # TP=1 (leverage 192GB memory)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
    qwen:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 1            # 8/(4*2) = 1
        gradient_accumulation_steps: 128 # 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 1    # TP=1 (leverage 192GB memory)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
  
  # Truly Identical: All platforms use exactly the same parallelism settings.
  # Ensures apples-to-apples comparison across hardware.
  # Same TP/PP/DP means identical compute graph and communication patterns.
  # Trade-off: May not fully utilize platform advantages (AMD's larger mem).
  truly_identical: # 01
    llama:
      nvidia:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
        data_parallel_size: 2
        gradient_accumulation_steps: 64
      amd:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
        data_parallel_size: 2
        gradient_accumulation_steps: 64
    qwen:
      nvidia:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
        data_parallel_size: 2
        gradient_accumulation_steps: 64
      amd:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
        data_parallel_size: 2
        gradient_accumulation_steps: 64 
  
  # Memory Optimized: Prioritizes reducing memory footprint per GPU.
  # Higher TP and PP to shard model weights and activations across GPUs.
  # Enables training larger models or larger batch sizes within memory.
  # Trade-off: Increased communication overhead reduces raw throughput.
  # Note: TP must divide attention heads evenly (Llama:32, Qwen:28 heads).
  memory_optimized: # 02
    llama:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4 (reduces mem, avoids excess comm)
        pipeline_model_parallel_size: 2  # PP=2 (further reduce memory per GPU)
        data_parallel_size: 1            # 8/(4*2) = 1
        gradient_accumulation_steps: 128 # 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 2    # TP=2 (conservative given 192GB)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # 8/(2*1) = 4
        gradient_accumulation_steps: 32  # 128/(1*4) = 32
    qwen:
      nvidia:
        tensor_model_parallel_size: 4    # TP=4 (max compatible with 28 heads)
        pipeline_model_parallel_size: 2  # PP=2 (further reduce memory per GPU)
        data_parallel_size: 1            # 8/(4*2) = 1
        gradient_accumulation_steps: 128 # 128/(1*1) = 128
      amd:
        tensor_model_parallel_size: 2    # TP=2 (28 heads / 2 = 14 per GPU)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # 8/(2*1) = 4
        gradient_accumulation_steps: 32  # 128/(1*4) = 32
  
  # Minimal Communication: Eliminates model parallelism to minimize GPU comm.
  # TP=1, PP=1 means entire model fits on each GPU (no sharding).
  # Maximum DP=8 for highest data throughput with only gradient all-reduce.
  # Trade-off: Requires sufficient GPU memory; best for smaller models.
  minimal_communication: # 03
    llama:
      nvidia:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel comm)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
      amd:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel comm)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
    qwen:
      nvidia:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel comm)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
      amd:
        tensor_model_parallel_size: 1    # TP=1 (no tensor parallel comm)
        pipeline_model_parallel_size: 1  # PP=1 (no pipeline bubbles)
        data_parallel_size: 8            # 8/(1*1) = 8
        gradient_accumulation_steps: 16  # 128/(1*8) = 16
  
  # Balanced: Uses moderate tensor parallelism (TP=2) providing trade-off:
  # - Memory efficiency: TP=2 reduces memory per GPU by splitting weights
  # - Communication overhead: Lower TP means less all-reduce communication
  # - Compute utilization: Not over-sharding, each GPU has meaningful work
  balanced: # 04
    llama:
      nvidia:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # 8/(2*1) = 4
        gradient_accumulation_steps: 32  # 128/(1*4) = 32
      amd:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 1  # PP=1
        data_parallel_size: 4            # 8/(2*1) = 4
        gradient_accumulation_steps: 32  # 128/(1*4) = 32
    qwen:
      nvidia:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 2            # 8/(2*2) = 2
        gradient_accumulation_steps: 64  # 128/(1*2) = 64
      amd:
        tensor_model_parallel_size: 2    # TP=2 (moderate sharding)
        pipeline_model_parallel_size: 2  # PP=2
        data_parallel_size: 2            # 8/(2*2) = 2
        gradient_accumulation_steps: 64  # 128/(1*2) = 64

platform_optimizations:
  nvidia:
    precision: "fp8"
    fp8_hybrid: true
    fp8_param: true
    cuda_alloc_conf: "expandable_segments:True"
    activation_checkpointing: true
    gradient_checkpointing: false
    
  amd:
    precision: "bf16"
    fp8_hybrid: false
    fp8_param: false
    activation_checkpointing: false
    gradient_checkpointing: false

benchmarking:
  output:
    directory: "./output"
    filename_format: "benchmark_{software_stack}_{model}.json"
    log_format: "training_{model}.log"
    
  metrics:
    performance:
      - "avg_step_time_seconds"
      - "tokens_per_second"
      - "tokens_per_second_per_gpu"
      - "steps_per_second"
      - "samples_per_second"
    
    memory:
      - "avg_memory_allocated_gb"
      - "peak_memory_allocated_gb"
      - "avg_memory_reserved_gb"
      - "peak_memory_reserved_gb"
    
    stability:
      - "step_time_std_dev"
      - "step_time_variance"
      - "min_step_time"
      - "max_step_time"
    
    system:
      - "device_name"
      - "device_count"
      - "gpu_cores"
      - "software_stack"
      - "software_version"
      - "pytorch_version"
  
  enhanced_metrics:
    enabled: true
    cloud_costs:
      nvidia_h100_8gpu_per_hour: 32.0
      amd_mi300x_8gpu_per_hour: 24.0
    hardware_specs:
      nvidia_h100:
        peak_tflops_fp8: 989.0e12
        peak_tflops_fp16: 494.0e12
        peak_tflops_bf16: 494.0e12
        tdp_watts: 700
      amd_mi300x:
        peak_tflops_fp16: 653.0e12
        peak_tflops_bf16: 653.0e12
        peak_tflops_fp8: 1300.0e12
        tdp_watts: 750
  
  warmup:
    skip_first_n_steps: 1           # Skip first step (warmup)
    
  runs:
    default_num_runs: 1             # For quick benchmarking
    recommended_num_runs: 3         # For statistical significance
    cooldown_seconds: 10            # Between runs

comparison:
  metrics_to_compare:
    - "tokens_per_second_per_gpu"
    - "avg_step_time_seconds"
    - "avg_memory_allocated_gb"
    - "mfu_percent"
    - "cost_per_trillion_tokens"
  plots:
    format: "png"
    dpi: 300
    style: "seaborn"
    filename: "compare.png"
  report:
    format: "markdown"
    filename: "comparison_report.md"
    include_plots: true

paths:
  primus:
    installation: "${PRIMUS_PATH:-/workspace/Primus}"
    config_dir: "examples/megatron/configs/MI300X"
    run_script: "./examples/run_pretrain.sh"
  nemo:
    checkpoint_dir: "/checkpoints"
  tprimat:
    base_dir: "."
    output_dir: "./output"
    logs_dir: "./output"

logging:
  console:
    enabled: true
    level: "INFO"
    colored: true
  file:
    enabled: true
    capture_stdout: true
    capture_stderr: true
  disable:
    tensorboard: true
    wandb: true
    mlflow: true

profiling:
  enabled: true                    # Enable NVIDIA Nsight Systems profiling
  trace: "cuda,nvtx,osrt,cudnn,cublas"  # What to trace
  cuda_memory_usage: true           # Track CUDA memory allocations
  capture_range: "cudaProfilerApi"  # Capture range method
  stats: true                       # Generate summary statistics
  force_overwrite: true             # Overwrite existing profiles
  export_json: true                 # Export to JSON (Chrome tracing)
