# Log Storage Guide

## Overview

This guide explains where and how logs are stored in the benchmarking system.

## Directory Structure

```
week-02/code/
├── benchmark_results/              ← Main output directory
│   ├── benchmark_cuda_*.json       ← NVIDIA benchmark results
│   ├── benchmark_rocm_*.json       ← AMD benchmark results
│   ├── primus_training_*.log       ← Primus training logs (if using wrapper)
│   ├── comparison_plot.png         ← Generated comparison charts
│   └── comparison_report.md        ← Generated comparison reports
│
├── profile_logs/                   ← PyTorch profiler output (if enabled)
│   └── perf_logs*.json             ← Chrome tracing format
│
└── *.log                           ← Direct training logs (if redirected)
```

## Log Types

### 1. Benchmark Results (JSON)

**Location**: `benchmark_results/benchmark_{platform}_{timestamp}.json`

**Size**: ~5-15 KB per run

**Format**: JSON

**Example filename**: `benchmark_rocm_20260105_112906.json`

**Content structure**:
```json
{
  "platform": "rocm",
  "gpu_info": {
    "device_count": 8,
    "device_name": "AMD Instinct MI300X",
    "total_memory_gb": 191.98,
    "platform": "rocm",
    "rocm_version": "6.0"
  },
  "timestamp": "2026-01-05T11:29:06",
  "training_config": {
    "max_steps": 10,
    "global_batch_size": 128,
    "sequence_length": 2048,
    "num_gpus": 8
  },
  "performance_metrics": {
    "total_steps": 10,
    "avg_step_time_seconds": 9.754,
    "tokens_per_second": 107501,
    "tokens_per_second_per_gpu": 13438
  },
  "memory_metrics": {
    "avg_memory_allocated_gb": 117.99,
    "peak_memory_allocated_gb": 117.99
  },
  "raw_step_times": [9.836, 9.702, 9.699, ...]
}
```

**Retention**: Keep indefinitely (small size)

### 2. Training Logs (Text)

**Location**: Depends on how you run training

**Size**: Varies (typically 100 KB - 10 MB)

**Format**: Plain text

**Storage options**:

#### Option A: Using Primus wrapper script
```bash
./run_primus_with_benchmark.sh
```
Saves to: `benchmark_results/primus_training_{timestamp}.log`

#### Option B: Manual redirect
```bash
EXP=... bash ./examples/run_pretrain.sh ... 2>&1 | tee training.log
```
Saves to: `training.log` (your specified location)

#### Option C: Default stdout
```bash
EXP=... bash ./examples/run_pretrain.sh ...
```
**Not saved** - only displayed on screen

**Content example** (Primus format):
```
[20260105 11:27:48][rank-7/8][INFO] iteration 2/10 | consumed samples: 256 | 
elapsed time per iteration (ms): 9836.3/21761.7 | 
hip mem usage: 117.99GB | tokens per GPU: 13325.3 | ...
```

**Retention**: Delete after extracting metrics (large size)

### 3. Comparison Reports

**Location**: `benchmark_results/`

**Generated by**: `compare_results.py`

#### A. Comparison Plot (PNG)
- **File**: `comparison_plot.png`
- **Size**: 100-500 KB
- **Content**: 6-panel visual comparison
- **Regenerated**: Each time you run comparison

#### B. Comparison Report (Markdown)
- **File**: `comparison_report.md`
- **Size**: 5-10 KB
- **Content**: Detailed text analysis
- **Regenerated**: Each time you run comparison

### 4. PyTorch Profiler Logs (Optional)

**Location**: `profile_logs/` (if profiler enabled)

**Size**: 50-500 MB per run

**Format**: JSON (Chrome tracing format)

**Content**: Kernel-level profiling data

**View with**: `chrome://tracing`

**Note**: Not enabled by default (adds overhead)

## Storage Workflow

### When Using Primus

```
1. Run Training
   ↓
   Primus logs → stdout/stderr
   ↓
   (if using wrapper) → benchmark_results/primus_training_*.log
   
2. Extract Metrics
   ↓
   Read training log
   ↓
   Parse metrics
   ↓
   Save → benchmark_results/benchmark_rocm_*.json
   
3. Compare
   ↓
   Load both JSON files
   ↓
   Generate → benchmark_results/comparison_plot.png
           → benchmark_results/comparison_report.md
```

### When Using NeMo Recipes

```
1. Run Training
   ↓
   BenchmarkCallback → Real-time metric collection
   ↓
   on_train_end() → benchmark_results/benchmark_{platform}_*.json
   
2. Compare
   ↓
   (same as above)
```

## File Naming Convention

### Benchmark Results
```
benchmark_{platform}_{timestamp}.json

Components:
- platform: "cuda" or "rocm"
- timestamp: YYYYMMdd_HHmmss

Examples:
- benchmark_cuda_20260105_143022.json
- benchmark_rocm_20260105_112906.json
```

### Training Logs (if saved)
```
primus_training_{timestamp}.log
training_{date}_{experiment}.log
{model}_{date}.log
```

## Size Estimates

| File Type | Typical Size | When to Clean |
|-----------|--------------|---------------|
| Benchmark JSON | 5-15 KB | Never (keep all) |
| Training log | 1-10 MB | After extracting metrics |
| Comparison plot | 100-500 KB | Can regenerate anytime |
| Comparison report | 5-10 KB | Can regenerate anytime |
| Profiler logs | 50-500 MB | After analysis |

## Best Practices

### 1. **Always Save Training Logs** (for record-keeping)

```bash
# Good: Save log with timestamp
EXP=... bash ./examples/run_pretrain.sh ... \
    2>&1 | tee "training_$(date +%Y%m%d_%H%M%S).log"

# Better: Use wrapper that saves automatically
./run_primus_with_benchmark.sh
```

### 2. **Organize by Date or Experiment**

```bash
benchmark_results/
├── 2026-01-05/
│   ├── benchmark_cuda_20260105_143022.json
│   ├── benchmark_rocm_20260105_112906.json
│   └── training_logs/
│       ├── primus_training_112906.log
│       └── nemo_training_143022.log
```

### 3. **Clean Up Large Files**

```bash
# After extracting metrics, compress training logs
gzip benchmark_results/*.log

# Or delete if metrics are extracted
rm benchmark_results/*.log

# Keep JSON files (they're tiny)
```

### 4. **Version Control Considerations**

```bash
# .gitignore
benchmark_results/*.log         # Don't commit large logs
benchmark_results/*.png         # Can regenerate
profile_logs/                   # Too large
!benchmark_results/*.json       # DO commit benchmark JSONs
!benchmark_results/*.md         # DO commit reports
```

## Checking Storage Usage

```bash
# Check benchmark results size
du -sh benchmark_results/

# List files by size
ls -lhS benchmark_results/

# Count benchmark runs
ls benchmark_results/benchmark_*.json | wc -l

# Check specific file sizes
du -h benchmark_results/benchmark_*.json
du -h benchmark_results/*.log
```

## Example Storage After Multiple Runs

```
benchmark_results/
├── benchmark_cuda_20260105_143022.json      (8 KB)
├── benchmark_cuda_20260105_150134.json      (8 KB)
├── benchmark_cuda_20260105_152845.json      (8 KB)
├── benchmark_rocm_20260105_112906.json      (9 KB)
├── benchmark_rocm_20260105_134521.json      (9 KB)
├── benchmark_rocm_20260105_141256.json      (9 KB)
├── primus_training_112906.log              (2.3 MB)
├── primus_training_134521.log              (2.4 MB)
├── primus_training_141256.log.gz           (180 KB - compressed)
├── comparison_plot.png                      (245 KB)
└── comparison_report.md                     (6 KB)

Total: ~5 MB (with compressed logs)
```

## Accessing Logs

### View Latest Benchmark

```bash
# Latest CUDA result
ls -t benchmark_results/benchmark_cuda_*.json | head -1

# View contents
cat $(ls -t benchmark_results/benchmark_cuda_*.json | head -1) | python3 -m json.tool

# Extract specific metric
python3 -c "
import json
with open('$(ls -t benchmark_results/benchmark_cuda_*.json | head -1)') as f:
    data = json.load(f)
print(f\"Tokens/s/GPU: {data['performance_metrics']['tokens_per_second_per_gpu']:,.0f}\")
"
```

### View Training Log

```bash
# View last 50 lines
tail -n 50 benchmark_results/primus_training_*.log

# Search for errors
grep -i "error\|warning\|fail" benchmark_results/*.log

# Find step timings
grep "elapsed time per iteration" benchmark_results/primus_training_*.log
```

### Compare All Runs

```bash
# List all benchmarks
python3 -c "
import json, glob
for f in sorted(glob.glob('benchmark_results/benchmark_*.json')):
    with open(f) as fp:
        d = json.load(fp)
    print(f\"{d['platform']:5} {d['timestamp']:19} {d['performance_metrics'].get('tokens_per_second_per_gpu', 0):>10,.0f} tokens/s/GPU\")
"
```

## Backup Strategy

### Minimal Backup (Recommended)
```bash
# Backup only JSON files (tiny)
tar -czf benchmarks_backup_$(date +%Y%m%d).tar.gz \
    benchmark_results/benchmark_*.json \
    benchmark_results/comparison_report.md

# Size: ~50 KB for 10 runs
```

### Full Backup (if you want logs)
```bash
# Backup everything
tar -czf benchmarks_full_$(date +%Y%m%d).tar.gz \
    benchmark_results/

# Size: Varies (depends on logs)
```

### Restore
```bash
tar -xzf benchmarks_backup_20260105.tar.gz
```

## Automated Cleanup

Create `cleanup_old_logs.sh`:

```bash
#!/bin/bash
# Keep JSON files, delete logs older than 7 days

find benchmark_results/ -name "*.log" -mtime +7 -delete
find benchmark_results/ -name "*.log.gz" -mtime +30 -delete

echo "Cleaned up old training logs"
echo "Benchmark JSONs retained: $(ls benchmark_results/benchmark_*.json | wc -l)"
```

## Summary

| What | Where | Size | Keep? |
|------|-------|------|-------|
| **Benchmark results** | `benchmark_results/*.json` | 5-15 KB | ✅ Yes (forever) |
| **Training logs** | `benchmark_results/*.log` | 1-10 MB | ⚠️ Temporary (compress or delete after extraction) |
| **Comparison plots** | `benchmark_results/*.png` | 100-500 KB | ✅ Yes (can regenerate) |
| **Comparison reports** | `benchmark_results/*.md` | 5-10 KB | ✅ Yes (can regenerate) |
| **Profiler data** | `profile_logs/` | 50-500 MB | ⚠️ Temporary (delete after analysis) |

**Storage Philosophy**:
- **Keep**: Small JSON files with metrics (permanent record)
- **Compress**: Training logs (for debugging if needed)
- **Regenerate**: Comparison outputs (can recreate from JSONs)
- **Delete**: Large profiler traces (after analysis)

**Expected Storage**: ~100 KB per benchmark run (with logs compressed)

---

**Need to check your current storage?**
```bash
cd /workspace/support/week-02/code
du -sh benchmark_results/
ls -lh benchmark_results/
```

